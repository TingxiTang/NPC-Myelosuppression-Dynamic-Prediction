{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576436c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_working_dir = Path.cwd()\n",
    "\n",
    "# Model paths\n",
    "model_dir = current_working_dir.parent / \"models\"\n",
    "\n",
    "LR_models_paths = {\n",
    "    'Hb': f'{model_dir}/Hb_LR_Calibrated.joblib',\n",
    "    'PLT': f'{model_dir}/PLT_LR_Calibrated.joblib',\n",
    "    'WBC_Neut': f'{model_dir}/WBC_Neut_LR_Calibrated.joblib'\n",
    "}\n",
    "XGB_models_paths = {\n",
    "    'Hb': f'{model_dir}/Hb_XGBoost_Calibrated.joblib',\n",
    "    'PLT': f'{model_dir}/PLT_XGBoost_Calibrated.joblib',\n",
    "    'WBC_Neut': f'{model_dir}/WBC_Neut_XGBoost_Calibrated.joblib'\n",
    "}\n",
    "LGBM_models_paths = {\n",
    "    'Hb': f'{model_dir}/Hb_LightGBM_Calibrated.joblib',\n",
    "    'PLT': f'{model_dir}/PLT_LightGBM_Calibrated.joblib',\n",
    "    'WBC_Neut': f'{model_dir}/WBC_Neut_LightGBM_Calibrated.joblib'\n",
    "}\n",
    "TabPFN_models_paths = {\n",
    "    'Hb': f'{model_dir}/Hb_TabPFN_Calibrated.joblib',\n",
    "    'PLT': f'{model_dir}/PLT_TabPFN_Calibrated.joblib',\n",
    "    'WBC_Neut': f'{model_dir}/WBC_Neut_TabPFN_Calibrated.joblib'\n",
    "}\n",
    "\n",
    "# Scaler path\n",
    "scaler_path = f'{model_dir}/scaler_continuous.joblib'\n",
    "scaler_features_path = f'{model_dir}/scaler_continuous_features.joblib'\n",
    "\n",
    "# LR selected features\n",
    "LR_features = {\n",
    "    'Hb': f'{model_dir}/selected_features_Hb.csv',\n",
    "    'PLT': f'{model_dir}/selected_features_PLT.csv',\n",
    "    'WBC_Neut': f'{model_dir}/selected_features_WBC_Neut.csv'\n",
    "}\n",
    "\n",
    "# features aligned file paths\n",
    "XGB_features = f'{model_dir}/xgboost_feature_names.joblib'\n",
    "LGBM_features = f'{model_dir}/lightgbm_feature_names.joblib'\n",
    "TabPFN_features = f'{model_dir}/tabPFN_feature_names.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_file_path = current_working_dir.parent / \"data\" \n",
    "# Load datasets\n",
    "### BuildingCohort\n",
    "X_train = pd.read_parquet(f'{data_file_path}/X_train.parquet') # training set\n",
    "y_train = pd.read_parquet(f'{data_file_path}/y_train.parquet')\n",
    "X_test = pd.read_parquet(f'{data_file_path}/X_test.parquet') # tuning set\n",
    "y_test = pd.read_parquet(f'{data_file_path}/y_test.parquet')\n",
    "X_val = pd.read_parquet(f'{data_file_path}/X_val.parquet') # validation set\n",
    "y_val = pd.read_parquet(f'{data_file_path}/y_val.parquet')\n",
    "\n",
    "### RetroCohort\n",
    "X_retro = pd.read_parquet(f'{data_file_path}/X_retro.parquet')\n",
    "y_retro = pd.read_parquet(f'{data_file_path}/y_retro.parquet')\n",
    "\n",
    "### ProsCohort\n",
    "X_pros = pd.read_parquet(f'{data_file_path}/X_pros.parquet')\n",
    "y_pros = pd.read_parquet(f'{data_file_path}/y_pros.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fa7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_map = {\n",
    "    \"Hb\": \"outcome_Hb\",\n",
    "    \"PLT\": \"outcome_PLT\",\n",
    "    \"WBC_Neut\": \"outcome_WBC_Neut\"\n",
    "}\n",
    "\n",
    "model_paths = {\n",
    "    \"LR\": LR_models_paths,\n",
    "    \"XGBoost\": XGB_models_paths,\n",
    "    \"LGBM\": LGBM_models_paths,\n",
    "    #\"TabPFN\": TabPFN_models_paths\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"X_train\": (X_train, y_train),\n",
    "    \"X_test\": (X_test, y_test),\n",
    "    \"X_val\": (X_val, y_val)\n",
    "}\n",
    "\n",
    "model_name_map = {\n",
    "    \"LR\": \"LR\",\n",
    "    \"XGBoost\": \"XGBoost\",\n",
    "    \"LGBM\": \"LightGBM\",\n",
    "    \"TabPFN\": \"TabPFN\"\n",
    "}\n",
    "\n",
    "model_cmaps = {\n",
    "    \"LR\": \"Blues\",\n",
    "    \"XGBoost\": \"Greens\",\n",
    "    \"LGBM\": \"Purples\",\n",
    "    \"TabPFN\": \"Oranges\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class PlattScalingCalibrator:\n",
    "    def __init__(self, base_model):\n",
    "        self.base_model = base_model\n",
    "        self.platt_lr = LogisticRegression(max_iter=1000)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if hasattr(self.base_model, \"predict_proba\"):\n",
    "            raw_probs = self.base_model.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            raw_probs = self.base_model.predict(X)\n",
    "        self.platt_lr.fit(raw_probs.reshape(-1, 1), y)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(self.base_model, \"predict_proba\"):\n",
    "            raw_probs = self.base_model.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            raw_probs = self.base_model.predict(X)\n",
    "        calibrated_probs = self.platt_lr.predict_proba(raw_probs.reshape(-1, 1))\n",
    "        return calibrated_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8d4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "plt.rcParams['font.sans-serif'] = ['Arial']\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "roc_colors = {\"X_train\": \"#1F77B4\", \"X_test\": \"#2CA02C\", \"X_val\": \"#D62728\"}\n",
    "roc_linestyles = {\"X_train\": \"-\", \"X_test\": \"--\", \"X_val\": \"-.\"}\n",
    "\n",
    "xgb_feature_names = joblib.load(XGB_features)\n",
    "lgbm_feature_names = joblib.load(LGBM_features)\n",
    "tabpfn_feature_names = joblib.load(TabPFN_features)\n",
    "\n",
    "scaler = joblib.load(scaler_path)\n",
    "scaler_feature_names = joblib.load(scaler_features_path)\n",
    "\n",
    "def get_prob(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    return model.predict(X)\n",
    "\n",
    "def apply_scaler(X):\n",
    "    X_out = X.copy()\n",
    "    X_scale = X_out.reindex(columns=scaler_feature_names)\n",
    "    X_scaled = scaler.transform(X_scale)\n",
    "    X_out.loc[:, scaler_feature_names] = X_scaled\n",
    "    return X_out\n",
    "\n",
    "def align_features(X, feature_names):\n",
    "    return X.reindex(columns=feature_names)\n",
    "\n",
    "def bootstrap_metrics(y_true, prob, cutoff, n_boot=1000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    stats = []\n",
    "    n = len(y_true)\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.integers(0, n, n)\n",
    "        y_b = y_true[idx]\n",
    "        p_b = prob[idx]\n",
    "        y_hat = (p_b >= cutoff).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_b, y_hat, labels=[0, 1]).ravel()\n",
    "        auc = roc_auc_score(y_b, p_b)\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "        sen = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spe = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "        stats.append([auc, acc, sen, spe, ppv, npv])\n",
    "    stats = np.array(stats)\n",
    "    metrics = [\"AUC\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\"]\n",
    "    out = []\n",
    "    for i, m in enumerate(metrics):\n",
    "        vals = stats[:, i]\n",
    "        mean = np.nanmean(vals)\n",
    "        lci, uci = np.nanpercentile(vals, [2.5, 97.5])\n",
    "        out.append(f\"{mean:.3f} ({lci:.3f}-{uci:.3f})\")\n",
    "    return pd.Series(out, index=metrics)\n",
    "\n",
    "def format_metrics_table(metrics_dict, outcome_key):\n",
    "    rows = []\n",
    "    for dataset_name in [\"X_train\", \"X_test\", \"X_val\"]:\n",
    "        for metric in [\"AUC\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\"]:\n",
    "            row = {\"index\": metric, \"Datasets\": dataset_name}\n",
    "            for model_name in [\"LR\", \"XGBoost\", \"LGBM\"]:\n",
    "                row[model_name] = metrics_dict[(outcome_key, model_name, dataset_name)][metric]\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)[[\"index\", \"LR\", \"XGBoost\", \"LGBM\", \"Datasets\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "prob_dict = {}\n",
    "\n",
    "for outcome_key, outcome_col in outcome_map.items():\n",
    "    for model_name, path_dict in model_paths.items():\n",
    "        model = joblib.load(path_dict[outcome_key])\n",
    "\n",
    "        probs = {}\n",
    "        for split_name, (X, y) in datasets.items():\n",
    "            X_use = apply_scaler(X)\n",
    "\n",
    "            if model_name == \"LR\":\n",
    "                selected = pd.read_csv(LR_features[outcome_key]).iloc[:, 0].tolist()\n",
    "                X_use = X_use[selected]\n",
    "\n",
    "            if model_name == \"XGBoost\":\n",
    "                X_use = align_features(X_use, xgb_feature_names)\n",
    "\n",
    "            if model_name == \"LGBM\":\n",
    "                X_use = align_features(X_use, lgbm_feature_names)\n",
    "\n",
    "            if model_name == \"TabPFN\":\n",
    "                X_use = align_features(X_use, tabpfn_feature_names)\n",
    "\n",
    "            probs[split_name] = get_prob(model, X_use)\n",
    "\n",
    "        prob_dict[(outcome_key, model_name)] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d93234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutoff Selection and Evaluation\n",
    "def pick_cutoff_by_sens(y_true, y_prob, target_sens=0.85):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    sens = tpr\n",
    "    spec = 1 - fpr\n",
    "    mask = sens >= target_sens\n",
    "    if mask.sum() == 0:\n",
    "        idx = np.argmax(sens)\n",
    "        return thresholds[idx]\n",
    "    idx = np.argmax(spec[mask])\n",
    "    return thresholds[mask][idx]\n",
    "\n",
    "def eval_compact(y_true, y_prob, cutoff):\n",
    "    y_pred = (y_prob >= cutoff).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "    return sens, spec, ppv, fp\n",
    "\n",
    "sens_targets = [0.75, 0.80, 0.85, 0.90]\n",
    "rows = []\n",
    "\n",
    "for outcome_key, outcome_col in outcome_map.items():\n",
    "    for model_name in model_paths.keys():\n",
    "        probs = prob_dict[(outcome_key, model_name)]\n",
    "        for s in sens_targets:\n",
    "            cutoff = pick_cutoff_by_sens(y_test[outcome_col].values, probs[\"X_test\"], target_sens=s)\n",
    "            for split_name, y_df in [(\"Train\", y_train), (\"Test\", y_test), (\"Val\", y_val)]:\n",
    "                sens, spec, ppv, fp = eval_compact(y_df[outcome_col].values, probs[f\"X_{split_name.lower()}\"], cutoff)\n",
    "                rows.append({\n",
    "                    \"Outcome\": outcome_key,\n",
    "                    \"Model\": model_name,\n",
    "                    \"Dataset\": split_name,\n",
    "                    \"Target_Sensitivity\": s,\n",
    "                    \"Sensitivity\": sens,\n",
    "                    \"Cutoff\": cutoff,\n",
    "                    \"Specificity\": spec,\n",
    "                    \"PPV\": ppv,\n",
    "                    \"FP_count\": fp\n",
    "                })\n",
    "\n",
    "cutoff_summary = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a189220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutoff loading\n",
    "cutoff = pd.read_csv(f'{model_dir}/Cutoff.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67adc441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# results directory\n",
    "results_file_path = current_working_dir.parent / \"results\"\n",
    "\n",
    "metrics_dict = {}\n",
    "\n",
    "roc_figs = {}\n",
    "cm_figs = {}\n",
    "\n",
    "for outcome_key, outcome_col in outcome_map.items():\n",
    "    for model_name, path_dict in model_paths.items():\n",
    "        model = joblib.load(path_dict[outcome_key])\n",
    "        probs = {}\n",
    "        for dataset_name, (X, y) in datasets.items():\n",
    "            X_use = apply_scaler(X)\n",
    "            if model_name == \"LR\":\n",
    "                selected = pd.read_csv(LR_features[outcome_key]).iloc[:, 0].tolist()\n",
    "                X_use = X_use[selected]\n",
    "            if model_name == \"XGBoost\":\n",
    "                X_use = align_features(X_use, xgb_feature_names)\n",
    "            if model_name == \"LGBM\":\n",
    "                X_use = align_features(X_use, lgbm_feature_names)\n",
    "            if model_name == \"TabPFN\":\n",
    "                X_use = align_features(X_use, tabpfn_feature_names)\n",
    "            probs[dataset_name] = get_prob(model, X_use)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        for dataset_name, (X, y) in datasets.items():\n",
    "            y_true = y[outcome_col].values\n",
    "            fpr, tpr, _ = roc_curve(y_true, probs[dataset_name])\n",
    "            auc = roc_auc_score(y_true, probs[dataset_name])\n",
    "            axes[0].plot(\n",
    "                fpr, tpr,\n",
    "                color=roc_colors[dataset_name],\n",
    "                linestyle=roc_linestyles[dataset_name],\n",
    "                lw=2.2,\n",
    "                label=f\"{dataset_name} AUC={auc:.3f}\"\n",
    "            )\n",
    "        axes[0].plot([0, 1], [0, 1], color=\"#4DBBD5\", lw=2, linestyle=\"--\", alpha=0.6)\n",
    "        axes[0].set_xlim(0, 1)\n",
    "        axes[0].set_ylim(0, 1)\n",
    "        axes[0].set_xlabel(\"False Positive Rate\")\n",
    "        axes[0].set_ylabel(\"True Positive Rate\")\n",
    "        axes[0].set_title(f\"{outcome_col} | {model_name} | ROC\")\n",
    "        axes[0].legend(loc=\"lower right\")\n",
    "        axes[0].grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        y_val_true = y_val[outcome_col].values\n",
    "        prob_val = probs[\"X_val\"]\n",
    "        prob_true, prob_pred = calibration_curve(y_val_true, prob_val, n_bins=10, strategy=\"quantile\")\n",
    "        axes[1].plot(prob_pred, prob_true, marker=\"o\", markersize=7, color=\"#00A087\", linewidth=2.2)\n",
    "        axes[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", lw=2, alpha=0.6)\n",
    "        x_min, x_max = prob_pred.min(), prob_pred.max()\n",
    "        y_min, y_max = prob_true.min(), prob_true.max()\n",
    "        x_pad = (x_max - x_min) * 0.15 if x_max > x_min else 0.02\n",
    "        y_pad = (y_max - y_min) * 0.15 if y_max > y_min else 0.02\n",
    "        axes[1].set_xlim(max(0, x_min - x_pad), min(1, x_max + x_pad))\n",
    "        axes[1].set_ylim(max(0, y_min - y_pad), min(1, y_max + y_pad))\n",
    "        axes[1].set_xlabel(\"Predicted Probability\")\n",
    "        axes[1].set_ylabel(\"Observed Proportion\")\n",
    "        axes[1].set_title(f\"{outcome_col} | {model_name} | Calibration\")\n",
    "        axes[1].grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        thresholds_dca = np.linspace(0.01, 0.99, 50)\n",
    "        net_benefit_model = []\n",
    "        for t in thresholds_dca:\n",
    "            tp = np.sum((prob_val >= t) & (y_val_true == 1))\n",
    "            fp = np.sum((prob_val >= t) & (y_val_true == 0))\n",
    "            nb = (tp / len(y_val_true)) - (fp / len(y_val_true)) * (t / (1 - t))\n",
    "            net_benefit_model.append(nb)\n",
    "        nb_all = (np.sum(y_val_true == 1) / len(y_val_true)) - (np.sum(y_val_true == 0) / len(y_val_true)) * (thresholds_dca / (1 - thresholds_dca))\n",
    "        axes[2].plot(thresholds_dca, net_benefit_model, color=\"#E64B35\", linewidth=2.5, label=\"Model\")\n",
    "        axes[2].plot(thresholds_dca, nb_all, color=\"black\", linestyle=\":\", linewidth=2, label=\"Treat All\")\n",
    "        axes[2].axhline(y=0, color=\"gray\", linestyle=\"-\", linewidth=2, label=\"Treat None\")\n",
    "        axes[2].set_ylim(-0.05, max(max(net_benefit_model), max(nb_all)) * 1.2)\n",
    "        axes[2].set_xlabel(\"Threshold Probability\")\n",
    "        axes[2].set_ylabel(\"Net Benefit\")\n",
    "        axes[2].set_title(f\"{outcome_col} | {model_name} | DCA\")\n",
    "        axes[2].legend(loc=\"upper right\")\n",
    "        axes[2].grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig_path = f\"{results_file_path}/BuildingCohort/ROC_{outcome_col}_{model_name}.pdf\"\n",
    "        plt.savefig(fig_path, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "        roc_figs[(outcome_col, model_name)] = fig\n",
    "\n",
    "        cutoff_key = f\"{outcome_key}_{model_name_map[model_name]}_Calibrated\"\n",
    "        cutoff_val = cutoff.loc[cutoff_key, \"CutOFF\"]\n",
    "        for dataset_name, (X, y) in datasets.items():\n",
    "            y_true = y[outcome_col].values\n",
    "            prob = probs[dataset_name]\n",
    "            metrics_dict[(outcome_key, model_name, dataset_name)] = bootstrap_metrics(y_true, prob, cutoff_val)\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    for i, (model_name, path_dict) in enumerate(model_paths.items()):\n",
    "        for j, (outcome_key, outcome_col) in enumerate(outcome_map.items()):\n",
    "            model = joblib.load(path_dict[outcome_key])\n",
    "            X_use = apply_scaler(X)\n",
    "            if model_name == \"LR\":\n",
    "                selected = pd.read_csv(LR_features[outcome_key]).iloc[:, 0].tolist()\n",
    "                X_use = X_use[selected]\n",
    "            if model_name == \"XGBoost\":\n",
    "                X_use = align_features(X_use, xgb_feature_names)\n",
    "            if model_name == \"LGBM\":\n",
    "                X_use = align_features(X_use, lgbm_feature_names)\n",
    "            prob = get_prob(model, X_use)\n",
    "            cutoff_key = f\"{outcome_key}_{model_name_map[model_name]}_Calibrated\"\n",
    "            cutoff_val = cutoff.loc[cutoff_key, \"CutOFF\"]\n",
    "            y_true = y[outcome_col].values\n",
    "            y_pred = (prob >= cutoff_val).astype(int)\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "            ax = axes[i, j]\n",
    "            ax.imshow(cm, cmap=model_cmaps[model_name])\n",
    "            ax.set_title(f\"{dataset_name} | {model_name} | {outcome_col}\")\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_yticks([0, 1])\n",
    "            for (r, c), v in np.ndenumerate(cm):\n",
    "                ax.text(c, r, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = f\"{results_file_path}/BuildingCohort/Confusion_{dataset_name.replace(' ', '_')}.pdf\"\n",
    "    plt.savefig(fig_path, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    cm_figs[dataset_name] = fig\n",
    "\n",
    "with pd.ExcelWriter(f\"{results_file_path}/BuildingCohort/sTable3_Results.xlsx\") as writer:\n",
    "    format_metrics_table(metrics_dict, \"Hb\").to_excel(writer, sheet_name=\"sTable3F. Hb Results\", index=False)\n",
    "    format_metrics_table(metrics_dict, \"PLT\").to_excel(writer, sheet_name=\"sTable3G. PLT Results\", index=False)\n",
    "    format_metrics_table(metrics_dict, \"WBC_Neut\").to_excel(writer, sheet_name=\"sTable3H. WBC_Neut Results\", index=False)\n",
    "\n",
    "results = {\n",
    "    \"roc_figs\": roc_figs,\n",
    "    \"metrics_tables\": metrics_dict,\n",
    "    \"confusion_figs\": cm_figs\n",
    "}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on External Cohorts\n",
    "\n",
    "calib_xlim = {\n",
    "    \"Prospective\": {\"Hb\": 0.15, \"PLT\": 0.20, \"WBC_Neut\": 0.50},\n",
    "    \"Retrospective\": {\"Hb\": 0.25, \"PLT\": 0.05, \"WBC_Neut\": 0.30}\n",
    "}\n",
    "\n",
    "def dca_curve(y_true, prob):\n",
    "    thresholds = np.linspace(0.01, 0.99, 50)\n",
    "    net_benefit_model = []\n",
    "    for t in thresholds:\n",
    "        tp = np.sum((prob >= t) & (y_true == 1))\n",
    "        fp = np.sum((prob >= t) & (y_true == 0))\n",
    "        nb = (tp / len(y_true)) - (fp / len(y_true)) * (t / (1 - t))\n",
    "        net_benefit_model.append(nb)\n",
    "    nb_all = (np.sum(y_true == 1) / len(y_true)) - (np.sum(y_true == 0) / len(y_true)) * (thresholds / (1 - thresholds))\n",
    "    return thresholds, np.array(net_benefit_model), nb_all\n",
    "\n",
    "def eval_metrics(y_true, prob, cutoff):\n",
    "    y_pred = (prob >= cutoff).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    auc = roc_auc_score(y_true, prob) if len(np.unique(y_true)) > 1 else np.nan\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sen = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "    spe = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
    "    return auc, acc, sen, spe, ppv, npv, fp\n",
    "\n",
    "def eval_one_cohort(X, y, cohort_label):\n",
    "    rows = []\n",
    "    for outcome_key, outcome_col in outcome_map.items():\n",
    "        model = joblib.load(LGBM_models_paths[outcome_key])\n",
    "        X_use = align_features(X, lgbm_feature_names)\n",
    "        prob = get_prob(model, X_use)\n",
    "        cutoff_key = f\"{outcome_key}_LightGBM_Calibrated\"\n",
    "        cutoff_val = cutoff.loc[cutoff_key, \"CutOFF\"]\n",
    "        auc, acc, sen, spe, ppv, npv, fp = eval_metrics(y[outcome_col].values, prob, cutoff_val)\n",
    "        rows.append({\n",
    "            \"Cohort\": cohort_label,\n",
    "            \"Outcome\": outcome_key,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Sensitivity\": sen,\n",
    "            \"Specificity\": spe,\n",
    "            \"PPV\": ppv,\n",
    "            \"NPV\": npv,\n",
    "            \"FP_count\": fp,\n",
    "            \"Cutoff\": cutoff_val\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_cohort_panel(X, y, cohort_label):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "    for i, (outcome_key, outcome_col) in enumerate(outcome_map.items()):\n",
    "        model = joblib.load(LGBM_models_paths[outcome_key])\n",
    "        X_use = align_features(X, lgbm_feature_names)\n",
    "        prob = get_prob(model, X_use)\n",
    "        y_true = y[outcome_col].values\n",
    "\n",
    "        if len(np.unique(y_true)) > 1:\n",
    "            fpr, tpr, _ = roc_curve(y_true, prob)\n",
    "            auc = roc_auc_score(y_true, prob)\n",
    "            axes[i, 0].plot(fpr, tpr, color=\"#D62728\", lw=2.2, label=f\"AUC={auc:.3f}\")\n",
    "        axes[i, 0].plot([0, 1], [0, 1], color=\"#4DBBD5\", lw=2, linestyle=\"--\", alpha=0.6)\n",
    "        axes[i, 0].set_xlim(0, 1)\n",
    "        axes[i, 0].set_ylim(0, 1)\n",
    "        axes[i, 0].set_xlabel(\"False Positive Rate\")\n",
    "        axes[i, 0].set_ylabel(\"True Positive Rate\")\n",
    "        axes[i, 0].set_title(f\"{outcome_col} | ROC\")\n",
    "        axes[i, 0].legend(loc=\"lower right\")\n",
    "        axes[i, 0].grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        prob_true, prob_pred = calibration_curve(y_true, prob, n_bins=10, strategy=\"quantile\")\n",
    "        axes[i, 1].plot(prob_pred, prob_true, marker=\"o\", markersize=7, color=\"#00A087\", linewidth=2.2)\n",
    "        axes[i, 1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", lw=2, alpha=0.6)\n",
    "        lim = calib_xlim[cohort_label][outcome_key]\n",
    "        axes[i, 1].set_xlim(0, lim)\n",
    "        axes[i, 1].set_ylim(0, lim)\n",
    "        axes[i, 1].set_xlabel(\"Predicted Probability\")\n",
    "        axes[i, 1].set_ylabel(\"Observed Proportion\")\n",
    "        axes[i, 1].set_title(f\"{outcome_col} | Calibration\")\n",
    "        axes[i, 1].grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        thresholds, nb_model, nb_all = dca_curve(y_true, prob)\n",
    "        axes[i, 2].plot(thresholds, nb_model, color=\"#E64B35\", linewidth=2.5, label=\"Model\")\n",
    "        axes[i, 2].plot(thresholds, nb_all, color=\"black\", linestyle=\":\", linewidth=2, label=\"Treat All\")\n",
    "        axes[i, 2].axhline(y=0, color=\"gray\", linestyle=\"-\", linewidth=2, label=\"Treat None\")\n",
    "        axes[i, 2].set_xlim(0, 0.5)\n",
    "        axes[i, 2].set_ylim(-0.02, max(max(nb_model), max(nb_all)) * 1.2)\n",
    "        axes[i, 2].set_xlabel(\"Threshold Probability\")\n",
    "        axes[i, 2].set_ylabel(\"Net Benefit\")\n",
    "        axes[i, 2].set_title(f\"{outcome_col} | DCA\")\n",
    "        axes[i, 2].legend(loc=\"upper right\")\n",
    "        axes[i, 2].grid(alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = f\"{results_file_path}/ExternalValidation/{cohort_label}_3x3.pdf\"\n",
    "    plt.savefig(fig_path, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "def plot_confusion_2x3(X_pros, y_pros, X_retro, y_retro):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    cohorts = [(\"Prospective\", X_pros, y_pros, \"Blues\"), (\"Retrospective\", X_retro, y_retro, \"Purples\")]\n",
    "    for r, (label, Xc, yc, cmap) in enumerate(cohorts):\n",
    "        for c, (outcome_key, outcome_col) in enumerate(outcome_map.items()):\n",
    "            model = joblib.load(LGBM_models_paths[outcome_key])\n",
    "            X_use = align_features(Xc, lgbm_feature_names)\n",
    "            prob = get_prob(model, X_use)\n",
    "            cutoff_key = f\"{outcome_key}_LightGBM_Calibrated\"\n",
    "            cutoff_val = cutoff.loc[cutoff_key, \"CutOFF\"]\n",
    "            y_true = yc[outcome_col].values\n",
    "            y_pred = (prob >= cutoff_val).astype(int)\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "            ax = axes[r, c]\n",
    "            ax.imshow(cm, cmap=cmap)\n",
    "            ax.set_title(f\"{label} | {outcome_col}\")\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_yticks([0, 1])\n",
    "            for (i, j), v in np.ndenumerate(cm):\n",
    "                ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = f\"{results_file_path}/ExternalValidation/Confusion_2x3.pdf\"\n",
    "    plt.savefig(fig_path, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "pros_table = eval_one_cohort(X_pros, y_pros, \"Prospective\")\n",
    "retro_table = eval_one_cohort(X_retro, y_retro, \"Retrospective\")\n",
    "\n",
    "pros_table.to_csv(f\"{results_file_path}/ExternalValidation/Prospective_Eval.csv\", index=False)\n",
    "retro_table.to_csv(f\"{results_file_path}/ExternalValidation/Retrospective_Eval.csv\", index=False)\n",
    "\n",
    "plot_cohort_panel(X_pros, y_pros, \"Prospective\")\n",
    "plot_cohort_panel(X_retro, y_retro, \"Retrospective\")\n",
    "plot_confusion_2x3(X_pros, y_pros, X_retro, y_retro)\n",
    "\n",
    "pros_table, retro_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Cohort Cutoff Grid Evaluation\n",
    "sens_targets = [0.75, 0.80, 0.85, 0.90]\n",
    "\n",
    "def run_cutoff_eval(X, y, cohort_label):\n",
    "    rows = []\n",
    "    for outcome_key, outcome_col in outcome_map.items():\n",
    "        model = joblib.load(LGBM_models_paths[outcome_key])\n",
    "        X_use = align_features(X, lgbm_feature_names)\n",
    "        prob = get_prob(model, X_use)\n",
    "        y_true = y[outcome_col].values\n",
    "        for s in sens_targets:\n",
    "            cutoff = pick_cutoff_by_sens(y_true, prob, target_sens=s)\n",
    "            sens, spec, ppv, fp = eval_compact(y_true, prob, cutoff)\n",
    "            rows.append({\n",
    "                \"Cohort\": cohort_label,\n",
    "                \"Outcome\": outcome_key,\n",
    "                \"Target_Sensitivity\": s,\n",
    "                \"Sensitivity\": sens,\n",
    "                \"Specificity\": spec,\n",
    "                \"PPV\": ppv,\n",
    "                \"FP_count\": fp,\n",
    "                \"Cutoff\": cutoff\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "pros_cutoff_table = run_cutoff_eval(X_pros, y_pros, \"Prospective\")\n",
    "retro_cutoff_table = run_cutoff_eval(X_retro, y_retro, \"Retrospective\")\n",
    "\n",
    "pros_cutoff_table.to_csv(f\"{results_file_path}/ExternalValidation/Prospective_CutoffGrid.csv\", index=False)\n",
    "retro_cutoff_table.to_csv(f\"{results_file_path}/ExternalValidation/Retrospective_CutoffGrid.csv\", index=False)\n",
    "\n",
    "pros_cutoff_table, retro_cutoff_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db055b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load External Cutoff Results\n",
    "ExterCutoff = pd.read_csv(f\"{model_dir}/ExternalCutoff.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0484c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_external_cutoff(X, y, cohort_label, cutoff_suffix):\n",
    "    rows = []\n",
    "    for outcome_key, outcome_col in outcome_map.items():\n",
    "        model = joblib.load(LGBM_models_paths[outcome_key])\n",
    "        X_use = align_features(X, lgbm_feature_names)\n",
    "        prob = get_prob(model, X_use)\n",
    "        cutoff_key = f\"{outcome_key}_LightGBM_{cutoff_suffix}\"\n",
    "        cutoff_val = ExterCutoff.loc[cutoff_key, \"CutOFF\"]\n",
    "        auc, acc, sen, spe, ppv, npv, fp = eval_metrics(y[outcome_col].values, prob, cutoff_val)\n",
    "        rows.append({\n",
    "            \"Cohort\": cohort_label,\n",
    "            \"Outcome\": outcome_key,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Sensitivity\": sen,\n",
    "            \"Specificity\": spe,\n",
    "            \"PPV\": ppv,\n",
    "            \"NPV\": npv,\n",
    "            \"FP_count\": fp,\n",
    "            \"Cutoff\": cutoff_val\n",
    "        })\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    out_df = out_df[[\"Cohort\", \"Outcome\", \"AUC\", \"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"FP_count\", \"Cutoff\"]]\n",
    "    return out_df\n",
    "\n",
    "def plot_confusion_2x3_external(X_pros, y_pros, X_retro, y_retro):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    cohorts = [(\"Prospective\", X_pros, y_pros, \"Blues\", \"Pros\"), (\"Retrospective\", X_retro, y_retro, \"Purples\", \"Retro\")]\n",
    "    for r, (label, Xc, yc, cmap, suffix) in enumerate(cohorts):\n",
    "        for c, (outcome_key, outcome_col) in enumerate(outcome_map.items()):\n",
    "            model = joblib.load(LGBM_models_paths[outcome_key])\n",
    "            X_use = align_features(Xc, lgbm_feature_names)\n",
    "            prob = get_prob(model, X_use)\n",
    "            cutoff_key = f\"{outcome_key}_LightGBM_{suffix}\"\n",
    "            cutoff_val = ExterCutoff.loc[cutoff_key, \"CutOFF\"]\n",
    "            y_true = yc[outcome_col].values\n",
    "            y_pred = (prob >= cutoff_val).astype(int)\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "            ax = axes[r, c]\n",
    "            ax.imshow(cm, cmap=cmap)\n",
    "            ax.set_title(f\"{label} | {outcome_col}\")\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_yticks([0, 1])\n",
    "            for (i, j), v in np.ndenumerate(cm):\n",
    "                ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = f\"{results_file_path}/Confusion_2x3_ExternalCutoff.pdf\"\n",
    "    plt.savefig(fig_path, format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "pros_ext = eval_with_external_cutoff(X_pros, y_pros, \"Prospective\", \"Pros\")\n",
    "retro_ext = eval_with_external_cutoff(X_retro, y_retro, \"Retrospective\", \"Retro\")\n",
    "\n",
    "pros_ext.to_csv(f\"{results_file_path}/ExternalValidation/Prospective_Eval_ExternalCutoff.csv\", index=False)\n",
    "retro_ext.to_csv(f\"{results_file_path}/ExternalValidation/Retrospective_Eval_ExternalCutoff.csv\", index=False)\n",
    "\n",
    "plot_confusion_2x3_external(X_pros, y_pros, X_retro, y_retro)\n",
    "\n",
    "pros_ext, retro_ext"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
